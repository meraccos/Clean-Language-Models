{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            batch_first=False)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size, output_size)\n",
    "        self.linear2 = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.linear1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.linear2.weight, nonlinearity='relu')\n",
    "            \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = self.linear2(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, retrieve all the book names and collect the contexes in a list\n",
    "files = glob.glob('books/*.txt')\n",
    "book_names = []          \n",
    "all_book_lines = []   \n",
    "\n",
    "for filename in files:\n",
    "    book_names.append(os.path.splitext(os.path.basename(filename))[0])\n",
    "    with open(filename, 'r') as f:\n",
    "        all_book_lines += f.readlines()\n",
    "\n",
    "# Put all the chars into a single list\n",
    "chars = [char for line in all_book_lines for char in line]\n",
    "v = list(set(chars))  # vocab\n",
    "n_vocab = len(v)\n",
    "v_idx = {v[i]:i for i in range(n_vocab)}    # vocab-index dictionary\n",
    "idx_v = {i:v[i] for i in range(n_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_oh(word):\n",
    "    idxs = torch.tensor([v_idx[l] for l in word])\n",
    "    oh = F.one_hot(idxs, n_vocab)\n",
    "    return oh\n",
    "\n",
    "def oh_to_char(oh):\n",
    "    idx = torch.argmax(oh, dim=1)\n",
    "    return idx_v[idx.item()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aVOı-lP\n",
      "tlç!lç.lkGl\"çxıÇbıGGıbaVp;g-lçLüyıkyıLrç pMAGl.ç-lklA!\n",
      "çfP!MaVVÖ;gLı!lkçtMPMçElH\n",
      "Eç!ü-!ĞçEüyĞPaVplŞlç.\n",
      "-G!lgPlkçüPĞ!Ğgç.ıPklk!\n",
      "rVmltĞçLı-ĞkPıkç-lAP\n",
      "çıgoıkçş\"BıVNP;\";gçıkPı-ĞkıçĞAıgBıaVÖıkçfoçGıgĞçxıo-Ğk!ıçXıyGı\"çLlçGlgrVğı-Ğgıçl-l.çişkGı!ĞçXıkçtıPf\"MVÜıgĞGçfbMk!MçGıEĞbıbçtl-\n",
      "!!laVcıkkıbç.fBlŞ\n",
      "g!lçyıL!ĞLç.\n",
      "-\n",
      "gB\n",
      "\"aVjıGlgıçblHblktlçlEĞbĞç;\"MkaV?ı-ıçtıgıçĞAtĞgçĞtıkçtly\n",
      "rVTtbıçıP.Ğgçolk!\n",
      "çfglçtĞ!bĞrVhilkçşBıGıgçEü-Algç.ı-Gçü-!ĞGrV:ç-ükç-lAG!lkçXıkĞE\n",
      "gç!lkbltl-aVƏĞkçHı\"ĞGçEü--ı-ĞGtıgçi;gıAçüP!ĞaVzıgĞgçfŞ!MçEĞkç!;XBıçElŞlçs!ı!ĞaVvfPPMç;tMgolçAşkıktıkç-lPGlçĞiıkrVQk!ĞkPıkçıtklg\n",
      "gçĞPGĞ!lç-MHMaVp;g-lglçGıgırçtlP!\n",
      "Ş\n",
      "G!lP\n",
      ".raaVöşk;AG;AçPfgıkĞgĞgçEĞkçtıgĞgçĞbbı-ırV?Ğ-PıkĞgĞçxıgoıLçfPlkçAĞÇçBlylgçoMk!MaVQAbıkçElkPlk\n",
      "gçfçl!\n",
      "!çoükıktĞg!ıgZVjfgklçxlXçşPi;\";çüyı!ĞçEĞkçLıAl!ıgaVƏĞkçl-PlgçMkMgMçHıy!ıgçE;XilglrVR;ÇĞç-übĞkıçylPGlPlgç.ıPEĞgıaVRlgĞŞĞç.lb\n",
      "A!\n",
      "ç!ıkAlL\n",
      "gĞçyıGrVTtıL!ıgçÇıPkĞ-lkçLĞtĞ-ĞgiĞgçfPMkaVuıkçLıbçLĞGĞç!üGı\"ç !ıç.\n",
      "P\n",
      "gBI.\n",
      "!\n",
      "PlçlPrVƏlAtlç.lblkçiş-PıkĞrçıgPıkçfPGl\"rVhŞ\n",
      "\"G\n",
      "Arçü-ç;tb;çĞAEıP"
     ]
    }
   ],
   "source": [
    "# n_hidden = 256\n",
    "# batch_size = 128\n",
    "# model = LSTM(n_vocab, n_hidden, n_vocab, num_layers=10)\n",
    "# model.load_state_dict(torch.load('models/model_31.pt'))\n",
    "\n",
    "# h, c = model.init_hidden(batch_size=batch_size)\n",
    "# X = str_to_oh('A').type(torch.float32).unsqueeze(0)\n",
    "# print(oh_to_char(X.squeeze(0)), end='')\n",
    "# for i in range(400):\n",
    "#     output, (h, c) = model(X, (h, c))\n",
    "#     probs = F.softmax(output[0], dim=0)\n",
    "#     char_idx = torch.multinomial(probs, 1).item() \n",
    "#     X = F.one_hot(torch.tensor([char_idx]), num_classes=n_vocab).type(torch.float32)\n",
    "#     print(idx_v[char_idx], end='')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "n_hidden = 256\n",
    "batch_size = 1\n",
    "model = LSTM(n_vocab, n_hidden, n_vocab, num_layers=10)\n",
    "model.load_state_dict(torch.load('models/model_15.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "h, c = model.init_hidden(batch_size=batch_size)\n",
    "X = str_to_oh('a').type(torch.float32).unsqueeze(0)  # Add a batch dimension\n",
    "print(oh_to_char(X[0]), end='')\n",
    "\n",
    "for i in range(1000):\n",
    "    output, (h, c) = model(X, (h, c))\n",
    "    probs = F.softmax(output[0, -1], dim=0)  # Take the last output and apply softmax\n",
    "    char_idx = torch.multinomial(probs, 1).item()  # Sample from the distribution\n",
    "    X = F.one_hot(torch.tensor([char_idx]), num_classes=n_vocab).type(torch.float32).unsqueeze(0)\n",
    "    print(oh_to_char(X[0]), end='')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
