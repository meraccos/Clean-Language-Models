{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import LSTM, CharDataset\n",
    "    \n",
    "\n",
    "class Nizami:\n",
    "    def __init__(self, model_path = None):\n",
    "        self.n_hidden = 64\n",
    "        self.batch_size = 256\n",
    "        self.block_size = 200\n",
    "        self.num_layers = 1\n",
    "        self.lr = 0.0001\n",
    "        \n",
    "        self.emb_size = 10\n",
    "        \n",
    "        self.step = 0\n",
    "        self.eval_freq = 200\n",
    "        self.model_save_freq = 5000\n",
    "        self.writer = None\n",
    "        self.process_books()\n",
    "        self.prepare_model(model_path)\n",
    "        \n",
    "    def prepare_model(self, model_path=None):\n",
    "        self.model = LSTM(self.n_vocab, self.n_hidden, \n",
    "                          self.emb_size, self.num_layers)\n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def process_books(self):  \n",
    "        with open('books/nizami.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.chars = list(text)\n",
    "        v = list(set(self.chars))\n",
    "        self.n_vocab = len(v)\n",
    "\n",
    "        stoi = {ch:i for i, ch in enumerate(v)}\n",
    "        itos = {i:ch for i, ch in enumerate(v)}\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda l: ''.join(itos[i] for i in l)\n",
    "        \n",
    "        self.data = torch.tensor(self.encode(text), dtype=torch.long)\n",
    "        \n",
    "        n = int(0.95 * len(self.data))\n",
    "        train_data = self.data[:n]\n",
    "        val_data = self.data[n:]\n",
    "        \n",
    "        train_dataset = CharDataset(train_data, self.block_size)\n",
    "        val_dataset = CharDataset(val_data, self.block_size)\n",
    "\n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def save_model(self):\n",
    "        if not os.path.exists('models/'):\n",
    "            os.makedirs('models/')\n",
    "        file_name = f'models/model_{self.step // self.model_save_freq}.pt'\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "        \n",
    "    def generate(self, n_gen_chars=150):\n",
    "        x = torch.tensor(0).view(1)\n",
    "        ix = [x.item()]\n",
    "\n",
    "        hidden = None\n",
    "        for _ in range(n_gen_chars):\n",
    "            x, hidden = self.model(x, hidden)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = torch.multinomial(x, 1).squeeze(0)\n",
    "            x = x.type(torch.int)\n",
    "            ix.append(x.item())\n",
    "        return self.decode(ix)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        losses = []\n",
    "        for x, y in self.val_dl:\n",
    "            output, (h, c) = self.model(x, None)\n",
    "            output = output.view(-1, self.n_vocab)\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(output, y).detach()\n",
    "            losses.append(loss)\n",
    "    \n",
    "        self.writer.add_scalar('Eval/loss', sum(loss) / len(loss), self.step)\n",
    "\n",
    "    def train(self, max_epoch):\n",
    "        # init the writer here to avoid redundant logging during generation\n",
    "        if self.writer == None:    \n",
    "            self.writer = SummaryWriter()    \n",
    "            \n",
    "        for epoch in range(max_epoch):\n",
    "            for x, y in tqdm(self.train_dl):\n",
    "                output, (h, c) = self.model(x, None)\n",
    "                output = output.view(-1, self.n_vocab)\n",
    "                y = y.view(-1)\n",
    "                loss = F.cross_entropy(output, y)\n",
    "                \n",
    "                self.writer.add_scalar('Loss/train', loss.detach(), self.step)\n",
    "                self.step += 1\n",
    "                \n",
    "                if self.step % self.model_save_freq == 0:\n",
    "                    self.save_model()\n",
    "                \n",
    "                if self.step % self.eval_freq == 0:\n",
    "                    self.evaluate()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = None\n",
    "nizami = Nizami(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 199/6216 [02:28<1:15:02,  1.34it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nizami\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(nizami\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nizami\u001b[39m.\u001b[39;49mtrain(\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m, in \u001b[0;36mNizami.train\u001b[0;34m(self, max_epoch)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate()\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    117\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mNizami.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, y)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     91\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mEval/loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39msum\u001b[39m(loss) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loss), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[39m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[39m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[39m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39miteration over a 0-d tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "nizami.optimizer = torch.optim.AdamW(nizami.model.parameters(), lr=0.01)\n",
    "nizami.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qabÄ±rlar, sÉ™fÃ§unu deyildir.\n",
      "Qurtulur incitdi durmadan artÄ±q.\"\n",
      "SÉ™nin bir mehtandan ona É™jdÉ™r qara,\n",
      "DÃ¼nyanÄ± damarÄ±n sÉ™ndÉ™din istÉ™n.\n",
      "MÉ™ni varmÄ±, gÃ¶zlÉ™yÉ™r hÉ™dbÉ™t bilÉ™r, daÅŸ kimi gÃ¶zÉ™l,\n",
      "DÉ™rdli dÃ¼rrÉ™ onu \"orduma insan\n",
      "BelÉ™ incahÉ™n oxuyarmÄ± gÃ¶rÃ¼ncÉ™.\n",
      "O yerÉ™ daÄŸmadÄ± dÃ¼yÃ¼n Ã¶ncÉ™ bir\n",
      "Axta ÅŸÉ™hri, yaxÅŸÄ± sÉ™nin taxtÄ±na,\n",
      "Tamah heyran deyil ki, sancaqlara qaÃ§ nur.\n",
      "ÃœzÃ¼nÉ™ Ã§Ä±raÄŸÄ±, uca hÉ™rÉ™kÉ™t,\n",
      "Bundan istÉ™rmiÅŸmÉ™z sÃ¼lhÉ™ uzaÄŸÄ±.\n",
      "\n",
      "AÄŸÄ±l dÉ™hÅŸÉ™dÉ™nsiz Ã¶zÃ¼n yuvarÄ±.\n",
      "DanÄ±ÅŸdÄ± yaranÄ±b kÉ™niz Ã¼z-Ã¼zÉ™.\n",
      "ÅžirinÉ™ soldurur bu hekamÄ±ÅŸdÄ±.\n",
      "Bir az bitdi, dilindÉ™ doÄŸru yaradan,\n",
      "Bu fÉ™lÉ™klÉ™ri vuruÅŸlarasa dara.\n",
      "Qohumut É™tÉ™yi ol bu o Ã§É™mÉ™n.\n",
      "Ã–z qoca bir qÉ™srin gediÅŸ tÉ™ÅŸÉ™kÉ™z\n",
      "Mevib kÃ¶lgÉ™nÉ™yin aÄŸsa Ã§Ä±xanda.\n",
      "Yola dayda baÅŸ da sÉ™ndÉ™n o pÉ™rvan,\n",
      "Ã‡Ã¶rÉ™k durarsan da ÅŸah yaradandÄ±:\n",
      "SovuÅŸkÉ™n yÃ¼z Ã¼Ã§Ã¼n bayraq Ã§É™kÉ™rÉ™k,\n",
      "ÅžirÉ™ can edÉ™rkÉ™n, kulunu olar?\n",
      "EÅŸÅŸÉ™si, \n"
     ]
    }
   ],
   "source": [
    "print(nizami.decode(nizami.generate(800)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
