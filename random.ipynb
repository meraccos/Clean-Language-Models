{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import LSTM, CharDataset\n",
    "    \n",
    "\n",
    "class Nizami:\n",
    "    def __init__(self, model_path = None):\n",
    "        self.n_hidden = 64\n",
    "        self.batch_size = 256\n",
    "        self.block_size = 200\n",
    "        self.num_layers = 1\n",
    "        self.lr = 0.0001\n",
    "        \n",
    "        self.emb_size = 10\n",
    "        \n",
    "        self.step = 0\n",
    "        self.eval_freq = 200\n",
    "        self.model_save_freq = 5000\n",
    "        self.writer = None\n",
    "        self.process_books()\n",
    "        self.prepare_model(model_path)\n",
    "        \n",
    "    def prepare_model(self, model_path=None):\n",
    "        self.model = LSTM(self.n_vocab, self.n_hidden, \n",
    "                          self.emb_size, self.num_layers)\n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def process_books(self):  \n",
    "        with open('books/nizami.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.chars = list(text)\n",
    "        v = list(set(self.chars))\n",
    "        self.n_vocab = len(v)\n",
    "\n",
    "        stoi = {ch:i for i, ch in enumerate(v)}\n",
    "        itos = {i:ch for i, ch in enumerate(v)}\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda l: ''.join(itos[i] for i in l)\n",
    "        \n",
    "        self.data = torch.tensor(self.encode(text), dtype=torch.long)\n",
    "        \n",
    "        n = int(0.95 * len(self.data))\n",
    "        train_data = self.data[:n]\n",
    "        val_data = self.data[n:]\n",
    "        \n",
    "        train_dataset = CharDataset(train_data, self.block_size)\n",
    "        val_dataset = CharDataset(val_data, self.block_size)\n",
    "\n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def save_model(self):\n",
    "        if not os.path.exists('models/'):\n",
    "            os.makedirs('models/')\n",
    "        file_name = f'models/model_{self.step // self.model_save_freq}.pt'\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "        \n",
    "    def generate(self, n_gen_chars=150):\n",
    "        x = torch.tensor(0).view(1)\n",
    "        ix = [x.item()]\n",
    "\n",
    "        hidden = None\n",
    "        for _ in range(n_gen_chars):\n",
    "            x, hidden = self.model(x, hidden)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = torch.multinomial(x, 1).squeeze(0)\n",
    "            x = x.type(torch.int)\n",
    "            ix.append(x.item())\n",
    "        return self.decode(ix)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        losses = []\n",
    "        for x, y in self.val_dl:\n",
    "            output, (h, c) = self.model(x, None)\n",
    "            output = output.view(-1, self.n_vocab)\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(output, y).detach()\n",
    "            losses.append(loss)\n",
    "    \n",
    "        self.writer.add_scalar('Eval/loss', sum(loss) / len(loss), self.step)\n",
    "\n",
    "    def train(self, max_epoch):\n",
    "        # init the writer here to avoid redundant logging during generation\n",
    "        if self.writer == None:    \n",
    "            self.writer = SummaryWriter()    \n",
    "            \n",
    "        for epoch in range(max_epoch):\n",
    "            for x, y in tqdm(self.train_dl):\n",
    "                output, (h, c) = self.model(x, None)\n",
    "                output = output.view(-1, self.n_vocab)\n",
    "                y = y.view(-1)\n",
    "                loss = F.cross_entropy(output, y)\n",
    "                \n",
    "                self.writer.add_scalar('Loss/train', loss.detach(), self.step)\n",
    "                self.step += 1\n",
    "                \n",
    "                if self.step % self.model_save_freq == 0:\n",
    "                    self.save_model()\n",
    "                \n",
    "                if self.step % self.eval_freq == 0:\n",
    "                    self.evaluate()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = None\n",
    "nizami = Nizami(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 199/6216 [02:28<1:15:02,  1.34it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nizami\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(nizami\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nizami\u001b[39m.\u001b[39;49mtrain(\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m, in \u001b[0;36mNizami.train\u001b[0;34m(self, max_epoch)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate()\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    117\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mNizami.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, y)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     91\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mEval/loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39msum\u001b[39m(loss) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loss), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[39m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[39m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[39m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39miteration over a 0-d tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "nizami.optimizer = torch.optim.AdamW(nizami.model.parameters(), lr=0.01)\n",
    "nizami.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qabırlar, səfçunu deyildir.\n",
      "Qurtulur incitdi durmadan artıq.\"\n",
      "Sənin bir mehtandan ona əjdər qara,\n",
      "Dünyanı damarın səndədin istən.\n",
      "Məni varmı, gözləyər hədbət bilər, daş kimi gözəl,\n",
      "Dərdli dürrə onu \"orduma insan\n",
      "Belə incahən oxuyarmı görüncə.\n",
      "O yerə dağmadı düyün öncə bir\n",
      "Axta şəhri, yaxşı sənin taxtına,\n",
      "Tamah heyran deyil ki, sancaqlara qaç nur.\n",
      "Üzünə çırağı, uca hərəkət,\n",
      "Bundan istərmişməz sülhə uzağı.\n",
      "\n",
      "Ağıl dəhşədənsiz özün yuvarı.\n",
      "Danışdı yaranıb kəniz üz-üzə.\n",
      "Şirinə soldurur bu hekamışdı.\n",
      "Bir az bitdi, dilində doğru yaradan,\n",
      "Bu fələkləri vuruşlarasa dara.\n",
      "Qohumut ətəyi ol bu o çəmən.\n",
      "Öz qoca bir qəsrin gediş təşəkəz\n",
      "Mevib kölgənəyin ağsa çıxanda.\n",
      "Yola dayda baş da səndən o pərvan,\n",
      "Çörək durarsan da şah yaradandı:\n",
      "Sovuşkən yüz üçün bayraq çəkərək,\n",
      "Şirə can edərkən, kulunu olar?\n",
      "Eşşəsi, \n"
     ]
    }
   ],
   "source": [
    "print(nizami.decode(nizami.generate(800)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
