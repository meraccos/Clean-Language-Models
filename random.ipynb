{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import LSTM, CharDataset\n",
    "    \n",
    "\n",
    "class Nizami:\n",
    "    def __init__(self, model_path = None):\n",
    "        self.n_hidden = 64\n",
    "        self.batch_size = 128\n",
    "        self.block_size = 200\n",
    "        self.num_layers = 1\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        self.emb_size = 8\n",
    "        \n",
    "        self.step = 0\n",
    "        self.eval_freq = 200\n",
    "        self.model_save_freq = 5000\n",
    "        self.writer = None\n",
    "        self.process_books()\n",
    "        self.prepare_model(model_path)\n",
    "        \n",
    "    def prepare_model(self, model_path=None):\n",
    "        self.model = LSTM(self.n_vocab, self.n_hidden, \n",
    "                          self.emb_size, self.num_layers)\n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "    \n",
    "    def process_books(self):  \n",
    "        with open('books/nizami.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        self.chars = list(text)\n",
    "        v = list(set(self.chars))\n",
    "        self.n_vocab = len(v)\n",
    "\n",
    "        stoi = {ch:i for i, ch in enumerate(v)}\n",
    "        itos = {i:ch for i, ch in enumerate(v)}\n",
    "        self.encode = lambda s: [stoi[c] for c in s]\n",
    "        self.decode = lambda l: ''.join(itos[i] for i in l)\n",
    "        \n",
    "        self.data = torch.tensor(self.encode(text), dtype=torch.long)\n",
    "        \n",
    "        n = int(0.95 * len(self.data))\n",
    "        train_data = self.data[:n]\n",
    "        val_data = self.data[n:]\n",
    "        \n",
    "        train_dataset = CharDataset(train_data, self.block_size)\n",
    "        val_dataset = CharDataset(val_data, self.block_size)\n",
    "\n",
    "        self.train_dl = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dl = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def save_model(self):\n",
    "        if not os.path.exists('models/'):\n",
    "            os.makedirs('models/')\n",
    "        file_name = f'models/model_{self.step // self.model_save_freq}.pt'\n",
    "        torch.save(self.model.state_dict(), file_name)\n",
    "        \n",
    "    def generate(self, n_gen_chars=150):\n",
    "        self.model.eval()\n",
    "        x = torch.tensor(0).view(1)\n",
    "        ix = [x.item()]\n",
    "\n",
    "        hidden = None\n",
    "        for _ in range(n_gen_chars):\n",
    "            x, hidden = self.model(x, hidden)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = torch.multinomial(x, 1).squeeze(0)\n",
    "            x = x.type(torch.int)\n",
    "            ix.append(x.item())\n",
    "        self.model.train()\n",
    "        return self.decode(ix)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        for x, y in self.val_dl:\n",
    "            output, (h, c) = self.model(x, None)\n",
    "            output = output.view(-1, self.n_vocab)\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(output, y).detach().item()\n",
    "            losses.append(loss)\n",
    "\n",
    "        self.writer.add_scalar('Eval/loss', sum(losses) / len(losses), self.step)\n",
    "        self.model.train()\n",
    "\n",
    "    def train(self, max_epoch):\n",
    "        # init the writer here to avoid redundant logging during generation\n",
    "        if self.writer == None:    \n",
    "            self.writer = SummaryWriter()    \n",
    "            \n",
    "        for epoch in range(max_epoch):\n",
    "            for x, y in tqdm(self.train_dl):\n",
    "                output, (h, c) = self.model(x, None)\n",
    "                output = output.view(-1, self.n_vocab)\n",
    "                y = y.view(-1)\n",
    "                loss = F.cross_entropy(output, y)\n",
    "                \n",
    "                self.writer.add_scalar('Loss/train', loss.detach(), self.step)\n",
    "                self.step += 1\n",
    "                \n",
    "                if self.step % self.model_save_freq == 0:\n",
    "                    self.save_model()\n",
    "                \n",
    "                if self.step % self.eval_freq == 0:\n",
    "                    self.evaluate()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = None\n",
    "nizami = Nizami(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12432/12432 [2:31:33<00:00,  1.37it/s]    \n"
     ]
    }
   ],
   "source": [
    "nizami.optimizer = torch.optim.AdamW(nizami.model.parameters(), lr=0.0001)\n",
    "nizami.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qabırlar, səfçunu deyildir.\n",
      "Qurtulur incitdi durmadan artıq.\"\n",
      "Sənin bir mehtandan ona əjdər qara,\n",
      "Dünyanı damarın səndədin istən.\n",
      "Məni varmı, gözləyər hədbət bilər, daş kimi gözəl,\n",
      "Dərdli dürrə onu \"orduma insan\n",
      "Belə incahən oxuyarmı görüncə.\n",
      "O yerə dağmadı düyün öncə bir\n",
      "Axta şəhri, yaxşı sənin taxtına,\n",
      "Tamah heyran deyil ki, sancaqlara qaç nur.\n",
      "Üzünə çırağı, uca hərəkət,\n",
      "Bundan istərmişməz sülhə uzağı.\n",
      "\n",
      "Ağıl dəhşədənsiz özün yuvarı.\n",
      "Danışdı yaranıb kəniz üz-üzə.\n",
      "Şirinə soldurur bu hekamışdı.\n",
      "Bir az bitdi, dilində doğru yaradan,\n",
      "Bu fələkləri vuruşlarasa dara.\n",
      "Qohumut ətəyi ol bu o çəmən.\n",
      "Öz qoca bir qəsrin gediş təşəkəz\n",
      "Mevib kölgənəyin ağsa çıxanda.\n",
      "Yola dayda baş da səndən o pərvan,\n",
      "Çörək durarsan da şah yaradandı:\n",
      "Sovuşkən yüz üçün bayraq çəkərək,\n",
      "Şirə can edərkən, kulunu olar?\n",
      "Eşşəsi, \n"
     ]
    }
   ],
   "source": [
    "print(nizami.decode(nizami.generate(800)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
